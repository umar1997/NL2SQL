{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b80c9f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 10:38:34.308112: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import argparse\n",
    "from packaging import version\n",
    "\n",
    "dir_path = os.path.dirname(os.path.realpath('./../'))\n",
    "sys.path.append(dir_path)\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertForTokenClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "\n",
    "from dataPreparation import Data_Preprocessing\n",
    "from dataProcessing import Data_Processing\n",
    "from log import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "356a389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_version = version.parse(transformers.__version__)\n",
    "assert pytorch_version >= version.parse('3.0.0'), \\\n",
    "    'We now only support transformers version >=3.0.0, but your version is {}'.format(pytorch_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfb78385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Dataset,DatasetDict\n",
    "from transformers import DataCollatorWithPadding,AutoModelForSequenceClassification, Trainer, TrainingArguments,AutoTokenizer,AutoModel,AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49fb460f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b8c75e9e7a4caa8e9d761cf9718feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2956c9d272ee45999066f4dee4283713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/462 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e85e46d0f914ca6b6a84968e3bd4165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1f8df76cf3435b9362afaf1ee3f7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='dmis-lab/biobert-v1.1', vocab_size=28996, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'dmis-lab/biobert-v1.1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b0b7cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d610d1c1355242e2a29f22c7a5a283b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/413M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f057449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDomainModel(nn.Module):\n",
    "    def __init__(self,checkpoint,num_labels): \n",
    "        super(CustomModel,self).__init__() \n",
    "        self.num_labels = num_labels \n",
    "\n",
    "        self.model = model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "        self.classifier = nn.Linear(768,num_labels) \n",
    "    def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
    "\n",
    "    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state\n",
    "\n",
    "    logits = self.classifier(sequence_output[:,0,:].view(-1,768)) # calculate losses\n",
    "    \n",
    "    loss = None\n",
    "    if labels is not None:\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "    \n",
    "    return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50353342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, HYPER_PARAMETERS, logger_progress, logger_results):\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        assert self.device == torch.device('cuda')\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case = False)\n",
    "\n",
    "        self.model = None\n",
    "        self.logger_progress = logger_progress\n",
    "        self.logger_results = logger_results\n",
    "        self.HYPER_PARAMETERS = HYPER_PARAMETERS\n",
    "\n",
    "        dataPreprocessing = Data_Preprocessing()\n",
    "        tokens, tags, tag_idx, tag_values = dataPreprocessing.main()\n",
    "        self.logger_progress.critical('Data Preprocessing Completed')\n",
    "        self.tokens, self.tags, self.tag_idx, self.tag_values = tokens, tags, tag_idx, tag_values\n",
    "        # Tokens and Tags are list of lists\n",
    "        # tag_idx is dictionary of encoding and tag value\n",
    "        # tag_value is list of all tags (domains) \n",
    "\n",
    "        dataProc = Data_Processing(tokens, tags, self.tokenizer, tag_idx, self.HYPER_PARAMETERS)\n",
    "        input_ids, tags, attention_masks = dataProc.getProcessedData()\n",
    "        # print('Inputs: {}'.format(input_ids[0]))\n",
    "        # print('Tags: {}'.format(tags[0]))\n",
    "        # print('Attention Mask: {}'.format(attention_masks[0]))\n",
    "        # print('Lengths Matching: {}, {}, {}'.format(len(input_ids[0]), len(tags[0]), len(attention_masks[0])))\n",
    "        self.logger_progress.critical('Data Processing Completed')\n",
    "\n",
    "        self.input_ids, self.tags, self.attention_masks = input_ids, tags, attention_masks\n",
    "        self.logger_progress.critical('Training Initialized!')\n",
    "\n",
    "    def train_test_split(self,):\n",
    "        # Get Train Test Split for Inputs and Tags\n",
    "        tr_input, val_input, tr_tag, val_tag = train_test_split(self.input_ids,self.tags,random_state=45,test_size=.15) \n",
    "        # Get Split for NER\n",
    "        tr_masks, val_masks, _, _ = train_test_split(self.attention_masks, self.input_ids, random_state=45, test_size=.15)\n",
    "\n",
    "        return tr_input, val_input, tr_tag, val_tag, tr_masks, val_masks\n",
    "\n",
    "    def convert_to_tensors(self, tr_input, val_input, tr_tag, val_tag, tr_masks, val_masks):\n",
    "\n",
    "        tr_input = torch.tensor(tr_input)\n",
    "        val_input = torch.tensor(val_input)\n",
    "\n",
    "        tr_tag = torch.tensor(tr_tag)\n",
    "        val_tag = torch.tensor(val_tag)\n",
    "\n",
    "        tr_masks = torch.tensor(tr_masks)\n",
    "        val_masks = torch.tensor(val_masks)\n",
    "\n",
    "        # print('Input Train Size: {}, {}, {}:'.format(len(tr_masks),len(tr_input), len(tr_tag)))\n",
    "        # print('Input Val Size: {}, {}, {}:'.format(len(val_masks),len(val_input), len(val_tag)))\n",
    "\n",
    "        return tr_input, val_input, tr_tag, val_tag, tr_masks, val_masks\n",
    "    \n",
    "    def data_loader(self, tr_input, val_input, tr_tag, val_tag, tr_masks, val_masks):\n",
    "\n",
    "        train_data = TensorDataset(tr_input, tr_masks, tr_tag)\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=self.HYPER_PARAMETERS['BATCH_SIZE'])\n",
    "\n",
    "        valid_data = TensorDataset(val_input, val_masks, val_tag)\n",
    "        valid_sampler = SequentialSampler(valid_data)\n",
    "        valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=self.HYPER_PARAMETERS['BATCH_SIZE'])\n",
    "\n",
    "        return train_dataloader, valid_dataloader\n",
    "\n",
    "    def model_init(self,):\n",
    "        # Getting BERT's pretrained Token Classification model\n",
    "        model = BertForTokenClassification.from_pretrained(\n",
    "        'bert-base-cased',\n",
    "        num_labels=len(self.tag_idx),\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False)\n",
    "\n",
    "        model.cuda()\n",
    "        self.model = model\n",
    "\n",
    "        # print(self.model)\n",
    "\n",
    "    def optimizer_and_lr_scheduler(self, train_dataloader):\n",
    "        FULL_FINETUNING = True\n",
    "        if FULL_FINETUNING: # Fine Tuning\n",
    "            param_optimizer = list(self.model.named_parameters())\n",
    "            no_decay = ['bias', 'gamma', 'beta']\n",
    "            optimizer_grouped_parameters = [\n",
    "                # Setting Weight Decay Rate 0.01 if it isnt bias, gamma and beta\n",
    "                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n",
    "                'weight_decay_rate': 0.01},\n",
    "                # If it is set to 0.0\n",
    "                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n",
    "                'weight_decay_rate': 0.0}\n",
    "            ]\n",
    "        else: # Non Fine Tuning\n",
    "            param_optimizer = list(self.model.classifier.named_parameters())\n",
    "            optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr= self.HYPER_PARAMETERS['LEARNING_RATE'],\n",
    "            eps= self.HYPER_PARAMETERS['EPSILON']\n",
    "        )\n",
    "        # Total number of training steps is number of batches * number of epochs.\n",
    "        total_steps = len(train_dataloader) * self.HYPER_PARAMETERS['EPOCHS']\n",
    "\n",
    "        # Create the learning rate scheduler.\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        return optimizer, scheduler\n",
    "\n",
    "\n",
    "    def training_and_validation(self, train_dataloader, valid_dataloader, optimizer, scheduler):\n",
    "\n",
    "        loss_values, validation_loss_values = [], []\n",
    "        E = 1\n",
    "        for _ in trange(self.HYPER_PARAMETERS['EPOCHS'], desc= \"Epoch \\n\"):\n",
    "            print('\\n')\n",
    "            print('     Epoch #{}'.format(E))\n",
    "            self.logger_results.info('Epoch #{}'.format(E))\n",
    "        \n",
    "            start = time.time()\n",
    "\n",
    "            self.model.train()\n",
    "            total_loss=0 # Reset at each Epoch\n",
    "            \n",
    "            ###################### TRAINING\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                batch = tuple(t.to(self.device) for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels = batch # Mantained the order for both train_data/val_data\n",
    "                \n",
    "                self.model.zero_grad() # Clearing previous gradients for each epoch\n",
    "                \n",
    "                outputs = self.model(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels) # Forward pass\n",
    "                \n",
    "                loss = outputs[0]\n",
    "                loss.backward() # Getting the loss and performing backward pass\n",
    "                \n",
    "                total_loss += loss.item() # Tracking loss\n",
    "                \n",
    "                # Preventing exploding grads\n",
    "                torch.nn.utils.clip_grad_norm_(parameters=self.model.parameters(), max_norm=self.HYPER_PARAMETERS['MAX_GRAD_NORM'])\n",
    "                \n",
    "                optimizer.step() # Updates parameters\n",
    "                scheduler.step() # Update learning_rate\n",
    "                \n",
    "            avg_train_loss = total_loss/len(train_dataloader) \n",
    "            print('     Average Train Loss For Epoch {}: {}'.format(E, avg_train_loss))\n",
    "            self.logger_results.info('Average Train Loss For Epoch {}: {}'.format(E, avg_train_loss))\n",
    "\n",
    "            loss_values.append(avg_train_loss) # Storing loss values to plot learning curve\n",
    "            ###################### VALIDATION\n",
    "            self.model.eval()\n",
    "            \n",
    "            eval_loss = 0\n",
    "            predictions, true_labels = [], []\n",
    "            \n",
    "            for batch in valid_dataloader:\n",
    "                batch = tuple(t.to(self.device)for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "                \n",
    "                with torch.no_grad(): # No backprop\n",
    "                    outputs = self.model(b_input_ids, token_type_ids =None,\n",
    "                                attention_mask=b_input_mask, labels=b_labels)\n",
    "                    \n",
    "                logits = outputs[1].detach().cpu().numpy() # Getting Probabilities for Prediction Classes\n",
    "                label_ids = b_labels.to('cpu').numpy() # Golden Labels\n",
    "                \n",
    "                loss = outputs[0]\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "                predictions.extend([list(p) for p in np.argmax(logits, axis=2)]) # Taking Max among Prediction Classes\n",
    "                true_labels.extend(label_ids)\n",
    "\n",
    "            avg_eval_loss = eval_loss / len(valid_dataloader)\n",
    "            print('     Average Val Loss For Epoch {}: {}'.format(E, avg_eval_loss))\n",
    "            self.logger_results.info('Average Val Loss For Epoch {}: {}'.format(E, avg_eval_loss))\n",
    "\n",
    "            validation_loss_values.append(avg_eval_loss)\n",
    "            \n",
    "            pred_tags = [self.tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                        for p_i, l_i, in zip(p,l)if self.tag_values[l_i] !='PAD']\n",
    "            \n",
    "            valid_tags = [self.tag_values[l_i]for l in true_labels\n",
    "                        for l_i in l if self.tag_values[l_i] !='PAD']\n",
    "            \n",
    "            print('     Validation Accuracy: {}%'.format(accuracy_score(pred_tags,valid_tags)*100))\n",
    "            print('     Validation F-1 Score:{}'.format(f1_score([pred_tags], [valid_tags])))\n",
    "\n",
    "            self.logger_results.info('Validation Accuracy: {}%  |  Validation F-1 Score:{}'.format(accuracy_score(pred_tags,valid_tags)*100, f1_score([pred_tags], [valid_tags])))\n",
    "\n",
    "            stop = time.time()\n",
    "            print('     Epoch #{} Duration:{}'.format(E, stop-start))\n",
    "            self.logger_results.info('Duration: {}\\n'.format(E, stop-start))\n",
    "            E+=1\n",
    "            print('-'*20)\n",
    "            time.sleep(3)\n",
    "\n",
    "    def run(self,):\n",
    "\n",
    "        tr_input, val_input, tr_tag, val_tag, tr_masks, val_masks = self.train_test_split()\n",
    "        tr_input, val_input, tr_tag, val_tag, tr_masks, val_masks = self.convert_to_tensors(tr_input, val_input, tr_tag, val_tag, tr_masks, val_masks)\n",
    "        train_dataloader, valid_dataloader = self.data_loader(tr_input, val_input, tr_tag, val_tag, tr_masks, val_masks)\n",
    "        self.model_init()\n",
    "        self.logger_progress.critical('Model Initialized!')\n",
    "        optimizer, scheduler = self.optimizer_and_lr_scheduler(train_dataloader)\n",
    "        self.logger_progress.critical('Starting Training. . .\\n')\n",
    "        self.training_and_validation(train_dataloader, valid_dataloader, optimizer, scheduler)\n",
    "        self.logger_progress.critical('Training Completed!')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24980c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # https://github.com/uf-hobi-informatics-lab/ClinicalTransformerNER/blob/master/src/run_transformer_ner.py\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--model_type\", default='bert', type=str, required=True,\n",
    "                        help=\"valid values: bert, _, _\")\n",
    "    parser.add_argument(\"--data_dir\", type=str,\n",
    "                        help=\"The input data directory.\")\n",
    "    parser.add_argument(\"--seed\", default=3, type=int,\n",
    "                        help='random seed')\n",
    "    parser.add_argument(\"--max_seq_length\", default=80, type=int,\n",
    "                        help=\"maximum number of tokens allowed in each sentence\")\n",
    "    parser.add_argument(\"--batch_size\", default=16, type=int,\n",
    "                        help=\"The batch size for training and evaluation.\")\n",
    "    parser.add_argument(\"--learning_rate\", default=3e-5, type=float,\n",
    "                        help=\"The initial learning rate for optimizer.\")\n",
    "    parser.add_argument(\"--num_epochs\", default=3, type=int,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
    "                        help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
    "                        help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\"--log_folder\", default='./Log_Files/', type=str,\n",
    "                        help=\"Name of log folder.\")\n",
    "    parser.add_argument(\"--log_file\", default='sample.log', type=str,\n",
    "                        help=\"Name of log file.\")\n",
    "\n",
    "    global_args = parser.parse_args()\n",
    "\n",
    "\n",
    "    HYPER_PARAMETERS = {\n",
    "        # \"MAX_LEN\" : 80, # Max Length of the sentence\n",
    "        # \"BATCH_SIZE\" : 16,\n",
    "        # \"EPOCHS\" : 3,\n",
    "        # \"MAX_GRAD_NORM\" : 1.0,\n",
    "        # \"LEARNING_RATE\" : 3e-5,\n",
    "        # \"EPSILON\" : 1e-8\n",
    "\n",
    "        \"MAX_LEN\" : global_args.max_seq_length, \n",
    "        \"BATCH_SIZE\" : global_args.batch_size,\n",
    "        \"EPOCHS\" : global_args.num_epochs,\n",
    "        \"MAX_GRAD_NORM\" : global_args.max_grad_norm,\n",
    "        \"LEARNING_RATE\" : global_args.learning_rate,\n",
    "        \"EPSILON\" : global_args.adam_epsilon\n",
    "        # \"TEST_SPLIT\": 0.15,\n",
    "        # \"RANDOM_SEED\": 42\n",
    "    }\n",
    "\n",
    "    file_name = global_args.log_folder + global_args.log_file\n",
    "    logger_meta = get_logger(name='META', file_name=file_name, type='meta')\n",
    "    logger_progress = get_logger(name='PORGRESS', file_name=file_name, type='progress')\n",
    "    logger_results = get_logger(name='RESULTS', file_name=file_name, type='results')\n",
    "\n",
    "    for i, (k, v) in enumerate(HYPER_PARAMETERS.items()):\n",
    "        if i == (len(HYPER_PARAMETERS) - 1):\n",
    "            logger_meta.warning(\"{}: {}\\n\".format(k, v))\n",
    "        else:\n",
    "            logger_meta.warning(\"{}: {}\".format(k, v))\n",
    "\n",
    "    print('Entity Classification Training')\n",
    "    print('------------------------------')\n",
    "    train = Training(HYPER_PARAMETERS, logger_progress, logger_results)\n",
    "    train.run()\n",
    "\n",
    "\n",
    "script = \"\"\"\n",
    "python entityClassification.py \\\n",
    "    --model_type bert \\\n",
    "    --data_dir ./../../Data/Chia_w_scope_data.csv \\\n",
    "    --max_seq_length 80 \\\n",
    "    --batch_size 16 \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_epochs 3 \\\n",
    "    --adam_epsilon 1e-8 \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --log_folder ./Log_Files/ \\\n",
    "    --log_file sample.log\n",
    "\"\"\"\n",
    "# python entityClassification.py \\\n",
    "#     --model_type bert \\\n",
    "#     --data_dir ./../../Data/Chia_w_scope_data.csv \\\n",
    "#     --max_seq_length 80 \\\n",
    "#     --batch_size 16 \\\n",
    "#     --learning_rate 5e-5 \\\n",
    "#     --num_epochs 5 \\\n",
    "#     --adam_epsilon 1e-8 \\\n",
    "#     --max_grad_norm 1.0 \\\n",
    "#     --log_folder ./Log_Files/ \\\n",
    "#     --log_file lr_5e-5.log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
