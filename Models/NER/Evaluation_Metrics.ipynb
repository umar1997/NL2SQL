{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cb15116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "import more_itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6a309d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = ['B-Condition', 'I-Condition', 'B-Drug', 'O','O','B-Drug','I-Drug','I-Drug','O', 'O']\n",
    "P = ['B-Condition', 'I-Condition', 'B-Temporal', 'O','O','O','I-Drug','O', 'O', 'O']\n",
    "\n",
    "len(A), len(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "215dc6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_idx = {'Drug': 0, 'O': 1, 'Device': 2, 'Negation': 3, 'Condition': 4, 'Qualifier': 5, 'Procedure': 6, 'Measurement': 7, 'Value': 8, 'Temporal': 9, 'PAD': 10, 'Observation': 11, 'Person': 12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4254e5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Drug', 1: 'O', 2: 'Device', 3: 'Negation', 4: 'Condition', 5: 'Qualifier', 6: 'Procedure', 7: 'Measurement', 8: 'Value', 9: 'Temporal', 10: 'PAD', 11: 'Observation', 12: 'Person'}\n"
     ]
    }
   ],
   "source": [
    "tag_values2idx = {i: t for i, t in enumerate(tag_idx)}\n",
    "print(tag_values2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c39de428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [0, 2, 1, 3, 2]\n",
    "y_true = [0, 1, 2, 3, 3]\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70dce53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4000000000000001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, y_pred, average='macro')\n",
    "f1_score(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f75ec2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Qualifier': 0,\n",
       " 'Procedure': 1,\n",
       " 'Measurement': 2,\n",
       " 'Observation': 3,\n",
       " 'Condition': 4,\n",
       " 'Temporal': 5,\n",
       " 'Drug': 6,\n",
       " 'Person': 7,\n",
       " 'Value': 8,\n",
       " 'Device': 9,\n",
       " 'Negation': 10,\n",
       " 'O': 11}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2Dict = {'Qualifier': 0, 'Procedure': 1, 'Measurement': 2, 'Observation': 3, 'Condition': 4, 'Temporal': 5, 'Drug': 6, 'Person': 7, 'Value': 8, 'Device': 9, 'Negation': 10, 'O':11}\n",
    "tag2Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e04f7b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Condition', 'Drug', 'O', 'Drug', 'O'],\n",
       " ['Condition', 'Temporal', 'O', 'Drug', 'O'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getPredictions(A, P):   \n",
    "    def getEntityIndices(A):   \n",
    "        entity_index = []\n",
    "        for i, a in enumerate(A):\n",
    "            if a.startswith('B-'):\n",
    "                if i == len(A)-1:\n",
    "                    entity_index.append((i,i,a[2:]))\n",
    "                else:\n",
    "                    for j, b in enumerate(A, start=i+1):\n",
    "                        if A[j].startswith('I-'):\n",
    "                            if j == (len(A)-1):\n",
    "                                entity_index.append((i,j,a[2:]))\n",
    "                                break\n",
    "                            pass\n",
    "                        else:\n",
    "                            entity_index.append((i,j-1,a[2:]))\n",
    "                            break\n",
    "            elif a == 'O':\n",
    "                if i == len(A)-1:\n",
    "                    entity_index.append((i,i,a))\n",
    "                else:\n",
    "                    for j, b in enumerate(A, start=i+1):\n",
    "                        if A[j] == 'O':\n",
    "                            if j == (len(A)-1):\n",
    "                                entity_index.append((i,j,a))\n",
    "                                break\n",
    "                            pass\n",
    "                        else:\n",
    "                            entity_index.append((i,j-1,a))\n",
    "                            break\n",
    "\n",
    "        return entity_index\n",
    "\n",
    "    def removeMultipleOs(C):\n",
    "        I = [i for i, c in enumerate(C) if c[2] == 'O']        \n",
    "        grplist = [list(group) for group in more_itertools.consecutive_groups(I)]\n",
    "        indexList = [grp[0] for grp in grplist]\n",
    "        removeList = []\n",
    "        for i, c in enumerate(C):\n",
    "            if (c[2] == 'O') and (i not in indexList):\n",
    "                removeList.append(i)\n",
    "        C = [i for j, i in enumerate(C) if j not in removeList]\n",
    "        return C\n",
    "\n",
    "    def getTrueLabels(entity_index):\n",
    "        true_label_entities = []\n",
    "        for entities in entity_index:\n",
    "            true_label_entities.append(entities[2])\n",
    "        return true_label_entities\n",
    "\n",
    "\n",
    "    def getPredLabels(P, entity_index):\n",
    "        prediction_entities = []\n",
    "        for j, entities in enumerate(entity_index):\n",
    "            boolFindOverlap = correctOverlap = False\n",
    "            temp = 'Blah'\n",
    "            for i in range(entities[0],entities[1]+1):\n",
    "                if P[i].startswith('B-') or P[i].startswith('I-'):\n",
    "                    boolFindOverlap = True \n",
    "                    temp = P[i][2:]\n",
    "                    if P[i][2:] == entities[2]:\n",
    "                        correctOverlap = True\n",
    "                        prediction_entities.append(entities[2])\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "            if boolFindOverlap == False and correctOverlap == False:\n",
    "                prediction_entities.append('O')\n",
    "            elif boolFindOverlap == True and correctOverlap == False:\n",
    "                prediction_entities.append(temp)\n",
    "\n",
    "            assert (j+1)==len(prediction_entities)\n",
    "\n",
    "        return prediction_entities\n",
    "    \n",
    "    entity_index = getEntityIndices(A)\n",
    "    entity_index = removeMultipleOs(entity_index)\n",
    "    true_label_entities = getTrueLabels(entity_index)\n",
    "    prediction_entities = getPredLabels(P, entity_index)\n",
    "    \n",
    "    assert len(true_label_entities) == len(prediction_entities)\n",
    "    return true_label_entities, prediction_entities\n",
    "    \n",
    "true_label_entities, prediction_entities = getPredictions(A, P)\n",
    "true_label_entities, prediction_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b74997a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReports(true_label_entities, prediction_entities):\n",
    "\n",
    "    def classifcationReport(true_label_entities, prediction_entities):\n",
    "        CR = classification_report(true_label_entities, prediction_entities)\n",
    "        CR_Dict = classification_report(true_label_entities, prediction_entities, output_dict=True)\n",
    "        return CR, CR_Dict\n",
    "    \n",
    "    def getConfusionMatrix(true_label_entities, prediction_entities, CR_Dict):\n",
    "        labels_ = list(CR_Dict.keys())[:-3]\n",
    "        confusionMatrix = confusion_matrix(true_label_entities, prediction_entities, labels=labels_)\n",
    "        return confusionMatrix\n",
    "    \n",
    "    def getAccuracy(confusionMatrix, CR_Dict):\n",
    "        labels = list(CR_Dict.keys())[:-3]\n",
    "        acc = np.diag(confusionMatrix)/confusionMatrix.sum(1)\n",
    "        \n",
    "        return labels, acc\n",
    "    \n",
    "    CR, CR_Dict = classifcationReport(true_label_entities, prediction_entities)\n",
    "    confusionMatrix = getConfusionMatrix(true_label_entities, prediction_entities, CR_Dict)\n",
    "    labels, acc = getAccuracy(confusionMatrix, CR_Dict)\n",
    "    print(CR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f8340fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Condition', 'Drug', 'O', 'Drug', 'O'],\n",
       " ['Condition', 'Temporal', 'O', 'Drug', 'O'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_label_entities ,prediction_entities = getPredictions(A, P)\n",
    "true_label_entities, prediction_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50770f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Condition       1.00      1.00      1.00         1\n",
      "        Drug       1.00      0.50      0.67         2\n",
      "           O       1.00      1.00      1.00         2\n",
      "    Temporal       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.80         5\n",
      "   macro avg       0.75      0.62      0.67         5\n",
      "weighted avg       1.00      0.80      0.87         5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umar.salman/G42/nlp2sqlEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/umar.salman/G42/nlp2sqlEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/umar.salman/G42/nlp2sqlEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/umar.salman/G42/nlp2sqlEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/umar.salman/G42/nlp2sqlEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/umar.salman/G42/nlp2sqlEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/tmp/ipykernel_2468050/904916893.py:15: RuntimeWarning: invalid value encountered in true_divide\n",
      "  acc = np.diag(confusionMatrix)/confusionMatrix.sum(1)\n"
     ]
    }
   ],
   "source": [
    "getReports(true_label_entities, prediction_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf4fdb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umar.salman/G42/nlp2sqlEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/umar.salman/G42/nlp2sqlEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/umar.salman/G42/nlp2sqlEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/umar.salman/G42/nlp2sqlEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/umar.salman/G42/nlp2sqlEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/umar.salman/G42/nlp2sqlEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Condition': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 1},\n",
       " 'Drug': {'precision': 1.0,\n",
       "  'recall': 0.5,\n",
       "  'f1-score': 0.6666666666666666,\n",
       "  'support': 2},\n",
       " 'O': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 2},\n",
       " 'Temporal': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n",
       " 'accuracy': 0.8,\n",
       " 'macro avg': {'precision': 0.75,\n",
       "  'recall': 0.625,\n",
       "  'f1-score': 0.6666666666666666,\n",
       "  'support': 5},\n",
       " 'weighted avg': {'precision': 1.0,\n",
       "  'recall': 0.8,\n",
       "  'f1-score': 0.8666666666666666,\n",
       "  'support': 5}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CR = classification_report(true_label_entities, prediction_entities)\n",
    "CR_Dict = classification_report(true_label_entities, prediction_entities, output_dict=True)\n",
    "CR_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ca87bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Condition', 'Drug', 'O', 'Temporal']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(CR_Dict.keys())[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d5ecd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 2]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
    "y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
    "confusionMatrix = confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
    "confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51700dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.         0.66666667]\n"
     ]
    }
   ],
   "source": [
    "print(np.diag(confusionMatrix)/confusionMatrix.sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f7fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/umar1997/Courses/blob/main/Data%20Science/Linear_Regression_%26_Bag_Of_Words_Model.ipynb\n",
    "# For SNS Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31d6ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = t_label = [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
    "tag_values = ['O', 'Procedure', 'Value', 'Qualifier', 'Device', 'Observation', 'Measurement', 'PAD', 'Condition', 'Temporal', 'Negation', 'Drug', 'Person']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abf637d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m zipList \u001b[38;5;241m=\u001b[39m [(tag_values[p],tag_values[t])  \u001b[38;5;28;01mfor\u001b[39;00m p, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pred, t_label) \u001b[38;5;28;01mif\u001b[39;00m tag_values[t]\u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPAD\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m pred, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mzipList)\n\u001b[1;32m      3\u001b[0m pred, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(pred), \u001b[38;5;28mlist\u001b[39m(labels)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "zipList = [(tag_values[p],tag_values[t])  for p, t in zip(pred, t_label) if tag_values[t]!= 'PAD']\n",
    "pred, labels = zip(*zipList)\n",
    "pred, labels = list(pred), list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a535eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc44fa6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
