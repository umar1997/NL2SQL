{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee766ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from more_itertools import locate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f164cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paths = {\n",
    "    'Data': './../Data/',\n",
    "    'Chia_w_scope': './../Data/chia_with_scope/',\n",
    "    'Chia_wo_scope': './../Data/chia_without_scope/',\n",
    "}\n",
    "\n",
    "\n",
    "entity_types = ['Condition', 'Drug', 'Procedure', 'Measurement', 'Observation', 'Person', 'Device', \\\n",
    "    'Value', 'Temporal', 'Qualifier', 'Negation']\n",
    "\n",
    "relation_type = ['OR', 'AND', 'Has_qualifier', 'Has_value', 'Has_negation', 'Has_temporal', 'Has_context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48eeeaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Preparation:\n",
    "    def __init__(self, path, relation_type, entity_types):\n",
    "        self.path = path\n",
    "        self.entity_types = entity_types\n",
    "        self.relation_type = relation_type\n",
    "        self.ignore_files = []\n",
    "        self.input_files = []\n",
    "        \n",
    "        print('Data_Preparation Initialized...')\n",
    "        \n",
    "    ###################################################################### HELPER FUNCTIONS\n",
    "        \n",
    "    def checkEntityValue(self, e):\n",
    "        \"\"\"\n",
    "        Helper Function for get_annotation_relations\n",
    "        \"\"\"\n",
    "        if e.startswith('T'):\n",
    "            return e.strip()\n",
    "        else:\n",
    "            return e.split(':')[-1].strip()\n",
    "        \n",
    "    \n",
    "    def removePunctuation(self, word):\n",
    "        \"\"\"\n",
    "        Helper Function for get_Entity_Tags\n",
    "        \"\"\"\n",
    "        word = re.sub(r'^(\\.|,|\\(|\\))', '', word)\n",
    "        word = re.sub(r'(\\.|,|\\(|\\))$', '', word)\n",
    "        return word\n",
    "    \n",
    "    def cleanEntityTokens(self, txt):\n",
    "        \"\"\"\n",
    "        Clean the tokens from / and - for words in entities\n",
    "        \"\"\"\n",
    "        txt = txt.replace('/',' ')\n",
    "        txt = txt.replace('-',' ')\n",
    "        txt = txt.replace(';',' ')\n",
    "        txt = re.sub(r' {2,}', ' ',txt)\n",
    "        txt = txt.strip()\n",
    "        \n",
    "        return txt.split()\n",
    "    \n",
    "    def readTxt(self, txt_file):\n",
    "        with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            text_array = f.read()\n",
    "        print(text_array)\n",
    "        \n",
    "    def readAnn(self, ann_file):\n",
    "        with open(ann_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            text_array = f.read()\n",
    "        print(text_array)\n",
    "        \n",
    "    def printFiles(self, file_name, file):\n",
    "        \"\"\"\n",
    "        Helper function to print files without manual viewing them\n",
    "        \"\"\"\n",
    "        ann_file = f\"{Paths[file_name]}{file}.ann\"\n",
    "        txt_file = f\"{Paths[file_name]}{file}.txt\"\n",
    "        self.readAnn(ann_file)\n",
    "        self.readTxt(txt_file)\n",
    "    \n",
    "    \n",
    "    ######################################################################\n",
    "    \n",
    "    def getInputFiles(self, file_name):\n",
    "        \"\"\"\n",
    "        file_name: 'Chia_w_scope' or 'Chia_wo_scope'\n",
    "        \"\"\"\n",
    "        inputfiles = set()\n",
    "        for f in os.listdir(Paths[file_name]):\n",
    "            if f.endswith('.ann'):\n",
    "                inputfiles.add(f.split('.')[0].split('_')[0])\n",
    "        self.input_files = list(inputfiles)\n",
    "        return inputfiles\n",
    "\n",
    "    def ignoreFiles(self, text_array):\n",
    "        match = re.findall(r'^( {1,}\\n$|NA {0,}\\n$)',text_array[0])\n",
    "        if len(match):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def get_annotation_entities(self, ann_file):\n",
    "        \"\"\"\n",
    "        Get all entities which correspond to the entities mentioned in the entity types.\n",
    "        \"\"\"\n",
    "        entities = []\n",
    "        with open(ann_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith('T'):\n",
    "                    assert len(line.strip().split('\\t')) == 3\n",
    "                    entity_identity = line.strip().split('\\t')[0]\n",
    "                    entity_token = line.strip().split('\\t')[-1]\n",
    "                    \n",
    "                    if ';' in line.strip().split('\\t')[1]:\n",
    "                        line = line.replace(';',' ')\n",
    "                        term = line.strip().split('\\t')[1].split()\n",
    "                        term[-1] = str(max([int(v) for v in term[1:]]))\n",
    "                        term[1] = str(min([int(v) for v in term[1:]]))\n",
    "                    else:\n",
    "                        term = line.strip().split('\\t')[1].split()\n",
    "                    \n",
    "                    if (self.entity_types != None) and (term[0] not in self.entity_types): continue\n",
    "                    if int(term[-1]) <= int(term[1]):\n",
    "                        raise RuntimeError('Starting and Ending Indices are off.')\n",
    "                    entities.append((entity_identity, int(term[1]), int(term[-1]), term[0], entity_token))\n",
    "                    \n",
    "        return sorted(entities, key=lambda x: (x[2]))\n",
    "    \n",
    "    def remove_overlap_entities(self, sorted_entities):\n",
    "        \"\"\"\n",
    "        If you want to get the largest overlap of entity so two words don't have different entities\n",
    "        \n",
    "        Here we just use the uncommented part to get the unique entities which we are considering.\n",
    "        \"\"\"\n",
    "#         keep_entities = []\n",
    "#         for idx, entity in enumerate(sorted_entities):\n",
    "#             if idx == 0:\n",
    "#                 keep_entities.append(entity)\n",
    "#                 last_keep = entity\n",
    "#                 continue\n",
    "#             if entity[1] < last_keep[2]:\n",
    "#                 if entity[2]-entity[1] > last_keep[2]-last_keep[1]:\n",
    "#                     last_keep = entity\n",
    "#                     keep_entities[-1] = last_keep\n",
    "#             elif entity[1] == last_keep[2]:\n",
    "#                 last_keep = (last_keep[1], entity[2], last_keep[-1])\n",
    "#                 keep_entities[-1] = last_keep\n",
    "#             else:\n",
    "#                 last_keep = entity\n",
    "#                 keep_entities.append(entity)\n",
    "\n",
    "        keep_entities = sorted_entities\n",
    "\n",
    "        uniqueEntity = []        \n",
    "        for ent in keep_entities:\n",
    "            uniqueEntity.append(ent[0])\n",
    "\n",
    "        return keep_entities, uniqueEntity\n",
    "\n",
    "\n",
    "    # https://datagy.io/python-list-find-all-index/\n",
    "    def get_annotation_relations(self, ann_file, uniqueEntity):\n",
    "        \"\"\"\n",
    "        Gives all relations corresponding to the relations mentioned in relations_type\n",
    "        And make sure they are relations between entities that are mentioned in entity_types\n",
    "        \"\"\"\n",
    "        relations = []\n",
    "        with open(ann_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.startswith('R') or line.startswith('*'):\n",
    "                    assert len(line.strip().split('\\t')) == 2 # ['R1', 'Has_qualifier Arg1:T6 Arg2:T5']\n",
    "                    if line.strip().split('\\t')[1].split()[0] not in relation_type: continue\n",
    "\n",
    "                    rel = line.strip().split('\\t')[0]\n",
    "                    rel_type = line.strip().split('\\t')[1].split()[0]\n",
    "                    entities = line.strip().split('\\t')[1].split()[1:]\n",
    "                    entities= [self.checkEntityValue(e) for e in entities]\n",
    "                    entities = [e for e in entities if e in uniqueEntity]\n",
    "                    entities = ' '.join(entities)\n",
    "                    relations.append((rel, rel_type, entities))\n",
    "\n",
    "        return relations\n",
    "    \n",
    "    \n",
    "    def get_text(self, txt_file, file):\n",
    "        \"\"\"\n",
    "        Get raw text\n",
    "        \"\"\"\n",
    "        with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            text_array = f.readlines()\n",
    "            if file in ['NCT02348918_exc', 'NCT02348918_inc', 'NCT01735955_exc']: # Inconsistent offsets (Line breaks)\n",
    "                text = ' '.join([i.strip() for i in text_array])\n",
    "            else:\n",
    "                text = '  '.join([i.strip() for i in text_array])\n",
    "        \n",
    "        return text, text_array\n",
    "    \n",
    "    \n",
    "    def get_text_array(self,text_array):\n",
    "        \"\"\"\n",
    "        Get cleaned text in array form\n",
    "        \"\"\"\n",
    "        \n",
    "        globalText = []\n",
    "        offset = 0\n",
    "        for txt in text_array:\n",
    "            textlen = len(txt)\n",
    "            \n",
    "            txt = txt.replace('/',' ')\n",
    "            txt = txt.replace('-',' ')\n",
    "            txt = txt.replace(';',' ')\n",
    "            txt = re.sub(r' {2,}', ' ',txt)\n",
    "            txt = txt.replace('.\\n','')\n",
    "            txt = txt.replace('\\n', '')\n",
    "            txt = txt.strip()\n",
    "\n",
    "            globalText.append(([self.removePunctuation(w) for w in txt.split()], offset, offset + textlen)) \n",
    "            offset += textlen\n",
    "\n",
    "        return globalText\n",
    "    \n",
    "    def get_NER_Tags(self, globalText, keep_entities):\n",
    "        \"\"\"\n",
    "        Get NER Tags for each token in a sentence.\n",
    "        Then also return a list of the entity identity e.g. T1, T2 etc.\n",
    "        \"\"\"\n",
    "        WORDS, TAGS, IDENTITY = [], [], []\n",
    "        offset = 10\n",
    "        for text in globalText:\n",
    "            words = text[0]\n",
    "            tags = ['O']*len(words)\n",
    "            entity_identity = ['X']*len(words)\n",
    "            sent_indices = set()\n",
    "            for k in keep_entities:\n",
    "                # Using 10 as an offset for wrong offset entries in inc/exc files\n",
    "                # Only look at entities if start and stop indices with an offset match\n",
    "                if k[1] >= (text[1]-offset) and k[2] <= (text[2]+offset): \n",
    "                    clean_tokens = self.cleanEntityTokens(k[-1])\n",
    "                    break_down = [self.removePunctuation(v) for v in clean_tokens] # Get all the tokens (punc removed) from entities\n",
    "                    main_index = 0\n",
    "                    label = ''\n",
    "                    for i, w in enumerate(break_down): # Go over entity tokens\n",
    "                        indices = list(locate(words, lambda x: x == w)) # See if tokens is in sentence\n",
    "\n",
    "                        if i == 0:\n",
    "                            try:\n",
    "                                main_index = indices[0]\n",
    "                                if len(break_down) > 1: label = 'B-'\n",
    "                            except:\n",
    "                                if len(w) <= offset:\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    pass\n",
    "#                                     raise RuntimeError('Word Length greater than offset')\n",
    "                        else:\n",
    "                            label = 'I-'\n",
    "                        indices= list(filter(lambda x: x >= main_index, indices))\n",
    "                        indices= list(filter(lambda x: x not in sent_indices, indices))\n",
    "                        if len(indices) != 0:\n",
    "                            sent_indices.add(indices[0])\n",
    "                            tags[indices[0]] = label + k[3]\n",
    "                            entity_identity[indices[0]] = k[0]\n",
    "            assert len(words) == len(tags) == len(entity_identity)\n",
    "            WORDS.append(words)\n",
    "            TAGS.append(tags) \n",
    "            IDENTITY.append(entity_identity)\n",
    "        return WORDS, TAGS, IDENTITY\n",
    "    \n",
    "    def save_to_df(self, FILES, CRITERIA, TEXT, GROUP_ENTITIES, RELATIONS, TOKENS, ENTITIES):\n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        df['File'] = pd.Series(FILES) \n",
    "        df['Criteria'] = pd.Series(CRITERIA) \n",
    "        df['Text'] = pd.Series(TEXT)\n",
    "        df['Group_Entities'] = pd.Series(GROUP_ENTITIES)\n",
    "        df['Relations'] = pd.Series(RELATIONS)\n",
    "        df['Tokens'] = pd.Series(TOKENS)\n",
    "        df['Entities'] = pd.Series(ENTITIES) # (Entity Name, Entity Identity)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def run(self, file_name):\n",
    "        \"\"\"\n",
    "        Run on all files\n",
    "        \"\"\"\n",
    "        inputfiles = self.getInputFiles(file_name)\n",
    "        \n",
    "        FILES, CRITERIA, TEXT, GROUP_ENTITIES, RELATIONS, TOKENS, ENTITIES = [], [], [], [], [], [], []\n",
    "        for infile in inputfiles:\n",
    "            for t in [\"inc\", \"exc\"]:\n",
    "                file = f\"{infile}_{t}\"\n",
    "                ann_file = f\"{Paths[file_name]}{file}.ann\"\n",
    "                txt_file = f\"{Paths[file_name]}{file}.txt\"\n",
    "                \n",
    "                text, text_array = self.get_text(txt_file, file)\n",
    "                ignore = self.ignoreFiles(text_array)\n",
    "                if ignore: \n",
    "                    self.ignore_files.append(file)\n",
    "                    continue\n",
    "                sorted_entities = self.get_annotation_entities(ann_file)\n",
    "                entities, uniqueEntity = self.remove_overlap_entities(sorted_entities)\n",
    "                relations = self.get_annotation_relations(ann_file, uniqueEntity)\n",
    "                global_text_array = self.get_text_array(text_array)\n",
    "                words, tags, entity_identity = self.get_NER_Tags(global_text_array, entities)           \n",
    "                \n",
    "                FILES.append(file)\n",
    "                CRITERIA.append(t)\n",
    "                TEXT.append(text_array)\n",
    "                GROUP_ENTITIES.append(entities)\n",
    "                RELATIONS.append(relations)\n",
    "                TOKENS.append(words)\n",
    "                ENTITIES.append((tags, entity_identity))\n",
    "#             break\n",
    "                           \n",
    "        df = self.save_to_df(FILES, CRITERIA, TEXT, GROUP_ENTITIES, RELATIONS, TOKENS, ENTITIES)       \n",
    "        return df\n",
    "                \n",
    "                \n",
    "    def main(self,files_name):\n",
    "        \"\"\"\n",
    "        Give list of files and from the files extract entities, tokens and relations and save in dataframe format\n",
    "        \"\"\"\n",
    "        \n",
    "        df = self.run(files_name)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05944c0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_Preparation Initialized...\n"
     ]
    }
   ],
   "source": [
    "dataPrep = Data_Preparation(Paths, relation_type, entity_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38de383f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "files = ['Chia_w_scope', 'Chia_wo_scope']\n",
    "df = dataPrep.main(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc4392ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NCT02969876_exc',\n",
       " 'NCT03088904_inc',\n",
       " 'NCT03088904_exc',\n",
       " 'NCT03134196_inc',\n",
       " 'NCT03134196_exc',\n",
       " 'NCT03082573_exc',\n",
       " 'NCT03389061_exc',\n",
       " 'NCT03350815_exc',\n",
       " 'NCT02042287_exc',\n",
       " 'NCT02222272_exc',\n",
       " 'NCT02953873_exc',\n",
       " 'NCT01490034_exc',\n",
       " 'NCT02785549_exc',\n",
       " 'NCT01352598_exc',\n",
       " 'NCT02918409_exc',\n",
       " 'NCT03045562_exc',\n",
       " 'NCT03106389_inc',\n",
       " 'NCT03106389_exc',\n",
       " 'NCT02944604_exc',\n",
       " 'NCT02715518_exc',\n",
       " 'NCT03011177_exc',\n",
       " 'NCT02504203_exc',\n",
       " 'NCT01742117_exc',\n",
       " 'NCT02408120_exc',\n",
       " 'NCT02748330_exc',\n",
       " 'NCT03372304_exc',\n",
       " 'NCT02478346_exc',\n",
       " 'NCT01944800_exc',\n",
       " 'NCT03077204_exc',\n",
       " 'NCT03198910_exc',\n",
       " 'NCT03278548_exc',\n",
       " 'NCT02323399_exc',\n",
       " 'NCT03156855_exc',\n",
       " 'NCT02256956_exc',\n",
       " 'NCT02270970_exc',\n",
       " 'NCT02607163_exc',\n",
       " 'NCT01088750_exc',\n",
       " 'NCT03263481_inc',\n",
       " 'NCT03263481_exc',\n",
       " 'NCT03620526_exc',\n",
       " 'NCT02219880_exc',\n",
       " 'NCT02457442_exc',\n",
       " 'NCT03355157_exc',\n",
       " 'NCT02399033_exc',\n",
       " 'NCT02590653_exc',\n",
       " 'NCT02620904_exc',\n",
       " 'NCT02256943_exc',\n",
       " 'NCT03228238_exc',\n",
       " 'NCT01078051_exc',\n",
       " 'NCT03615508_exc']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPrep.ignore_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "628b8305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1\tPerson 0 3\tAge\n",
      "T2\tValue 4 17\t<18 years old\n",
      "R1\tHas_value Arg1:T1 Arg2:T2\t\n",
      "T3\tPost-eligibility 19 75\tPatient unable to communicate or to understand the study\n",
      "T4\tPost-eligibility 77 121\tPatient refusing to participate to the study\n",
      "T5\tCondition 123 139\tcontraindication\n",
      "T6\tProcedure 143 154\tlaparoscopy\n",
      "R2\tAND Arg1:T5 Arg2:T6\t\n",
      "\n",
      "Age <18 years old\n",
      "Patient unable to communicate or to understand the study\n",
      "Patient refusing to participate to the study\n",
      "contraindication to laparoscopy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataPrep.printFiles('Chia_w_scope', 'NCT01346436_exc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83f3b679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1\tNon-query-able 0 2\tNA\n",
      "\n",
      "NA\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataPrep.printFiles('Chia_w_scope', 'NCT03615508_exc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b468fe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('main_data.csv')\n",
    "# xdf = pd.read_csv('main_data.csv')\n",
    "# xdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f87cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c2cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf.loc[0]['Entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55857726",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, f in xdf.iterrows():\n",
    "    print(f['Tokens'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de0392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, f in xdf.iterrows():\n",
    "    print(eval(f['Entities'])[0])\n",
    "    print(eval(f['Entities'])[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869947b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6678f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0250b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2aaa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
